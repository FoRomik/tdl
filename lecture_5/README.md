**This lecture:** 
Spin-glass model (https://arxiv.org/abs/1412.0233, http://proceedings.mlr.press/v40/Choromanska15.pdf).
Eliminating local minima (https://arxiv.org/abs/1901.00279).

[Slides](Loss_landscape_part3.pdf); [video](https://youtu.be/S9fZrOhcJ5s).

**Next lecture announcement:** 

Gradient descent (GD) almost surely converges to local minima for random initialization (https://arxiv.org/abs/1602.04915).
Example of loss function for which GD converges exponentially slow (https://arxiv.org/abs/1705.10412).
Noisy GD converges to local minima for any initialization (https://arxiv.org/abs/1503.02101) in polynomial time.
Noisy GD converges to global minima (https://core.ac.uk/download/pdf/4380833.pdf).

