**This lecture:** 

Gradient descent (GD) converges to a 1-st-order stationary point in dimension-free number of steps (https://mipt.ru/dcam/upload/abb/nesterovfinal-arpgzk47dcy.pdf).

GD almost surely doesn't converge to a given saddle point for random initialization (https://arxiv.org/abs/1602.04915).

Example of loss function for which GD converges exponentially slow (https://arxiv.org/abs/1705.10412).

Noisy GD converges to a 2nd-order stationary point (https://arxiv.org/abs/1503.02101) in dimensionality-polynomial number of steps.

Perturbed GD converges to a 2nd-order stationary point (https://arxiv.org/abs/1703.00887) in dimensionality-polylogarithmical number of steps.

Stochastic gradient Langevin dynamics (SGLD) converges to global minima in probability (https://core.ac.uk/download/pdf/4380833.pdf).

[Slides](GD_dynamics_part1.pdf); [video](https://youtu.be/RbJp94Ytyuo).

**Next lecture announcement:** 

GD converges to global minima in sufficiently wide nets with one hidden layer (https://openreview.net/forum?id=S1eK3i09YQ).

Generalization to multi-layer nets (https://arxiv.org/abs/1811.03804).
